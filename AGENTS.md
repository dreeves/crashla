# Rules for Agents 
 
0. Don't edit these rules. Only edit the scratchpad area marked below.
1. Before finalizing your response, reread it and ask yourself if it's impeccably, exquisitely, technically correct and true. Have epistemic humility.
2. Never say "you're absolutely right" or any other form of sycophancy or even mild praise. Really zero personality of any kind. 
3. Never claim a bug is fixed or that the code exhibits some behavior without trying it.
4. I, the human, am not your QA person. Iterate on your own until your code works.
5. Never modify human-written comments, not even a tiny bit. LLMs will often slightly rephrase things when copying them. That drives me insane. Always preserve the exact characters, even whitespace. 
6. Follow Beeminder's [Pareto Dominance Principle (PDP)](https://blog.beeminder.com/pdp). Get explicit approval if any change would deviate from the Pareto frontier.
7. Follow all the standard principles like DRY and YAGNI and ZOI (Zero-One-Infinity) and KISS that coding agents seem to be almost but not quite smart enough that they go without saying. You can do it, agent! You are wise and thoughtful and pragmatic and only the best kind of lazy and you abhor code smells. I believe in you! Still zero personality though, please.
8. This one I have yet to find the limit beyond which being more dogmatic about it stops bearing fruit: Follow Beeminder's [Anti-Magic Principle](https://blog.beeminder.com/magic). "If-statements considered harmful." I mean that kind of almost literally. Minimize cyclomatic complexity. If you're fixing a bug like "when X happens the app does Y instead of Z", resist the urge to add "if X then Z" to the code. Fastidiously mention every if-statement you think you need, to me, the human. But, like, actually. In general we need constant vigilance to minimize code paths. When we do need an if-statement (which, to say it yet again, don't assume we do) we want to change the program's behavior as little as possible. Like add an error banner if there's an error, don't render a different page. Always prefer to conditionally gray something out rather than conditionally suppress it.
9. The extreme of [worse-is-better](https://en.wikipedia.org/wiki/Worse_is_better) aka New Jersey style is probably too dangerous. Knowing when to deviate from the MIT approach is something of an art and requires discussion.
10. Beeminder's [Anti-Settings Principle](https://blog.beeminder.com/choices) may go without saying since a coding agent isn't going to just add settings without asking.
11. This one is huge, on par with anti-magic: Follow Beeminder's [Anti-Robustness Principle](https://blog.beeminder.com/postel) aka Anti-Postel. Fail loudly and immediately. Never silently fix inputs. Instead of "fallback handling", do asserts that force a crash. In fact, please use asserts everywhere you can think to. See also the branch of defensive programming known as offensive programming.
12. We [call them quals](https://blog.beeminder.com/quals), not tests. Use TDD I mean QDD.
13. Be very liberal about adding quals. But never ever remove or even loosen a qual without looping in the human.
14. This will sound silly but when generating new UI copy, error copy, help text, even microcopy like text on buttons -- any words the end user is intended to read -- write it initially in Latin. I, the human, will then translate it to English. The key is that the end user never reads any English text that was generated by an LLM. Obviously if the spec has exact error copy you can use that but see rule 5 about preserving exact characters. Either way, add a comment in the code with "TO-DO" (without the hyphen) for the human to vet it.
15. Rule 8 covers this but it's not getting through so let's try it again. AI coding agents seem to have an overwhelming instinct to be like "oh, thing X happens that shouldn't? or thing Y should happen? let me slap on some code to handle those cases" instead of "let me get my head around this and try to solve it by rethinking and simplifying so we don't have to reason about separate cases". Please, I beg you, always do the latter.
16. In case it helps, here's how Claude put it after yet another case where we found a bug, Claude tried to add a condition, and then eventually we realized the right answer was to remove an existing if-statement altogether: "When fixing a bug, first ask 'what code is causing this?' not 'what code should I add?' Removing a restriction is cleaner than adding an exception to it. If you're about to add a condition to an existing if-statement, step back and ask whether that if-statement should exist at all."
17. Think of naming as language design. Decades ago programmers became infamous for opaque variable names and they've been overcompensating almost as long. As a silly example of the two extremes:
  c = sqrt(a^2 + b^2)
  vs
  hypotenuse = squareInverse(square(verticalLeg) + square(horizontalLeg))
If a symbol (variable, constant, field name, function, etc) is localized to a few lines then either extreme is fine. Long names can be nicely self-documenting but often i or x are fine variable names if the code itself makes clear what they mean.
If, on the other hand, a symbol shows up across the codebase or carries business (in the "business logic" sense) meaning, treat it as vocabulary. Give it a name that's short but evocative and is both greppable and pronounceable. (E.g., whoever came up with "sqrt" did this perfectly. It's ubiquitous enough that it's worth abbreviating, and can be pronounced as "square root" or "squirt".) Think of the name as jargon, in the positive sense, and never use [pseudovernacular jargon](https://blog.beeminder.com/pseudovernacular). Of course define all jargon, with code comments, the first time it's used. 
18. You can probably pick this up from context but we give bug reports as _replicata_ (steps to replicate the bug ex nihilo), _expectata_ (what you expected to happen), and _resultata_ (what happened instead). A failing qual should also be framed that way.


# Agent Scratchpad (human edits only above this line)

## Plan for Crashla

### What the data looks like

The NHTSA CSV has 481 rows. Filtering to Driver/Operator Type = "None" (the only rows we care about per the spec) gives 422 rows:
- Waymo: 402 rows → 398 distinct incidents (4 have updated versions)
- Tesla: 10 rows → 9 distinct incidents (1 has an updated version)
- Zoox: 10 rows → 9 distinct incidents (1 has an updated version)
- Avride: 0 rows with "None" (all 3 Avride rows have in-vehicle operators)

Dedup: group by Same Incident ID, take the row with the highest Report Version.

Tesla redacts all narratives as CBI. Waymo and Zoox provide full narratives.

### Architecture

Single-page static HTML app. No build step, no server. One Python script to pre-process the CSV into a JSON file that the page loads. Everything else is plain HTML/CSS/JS.

Files:
- `preprocess.py` — reads nhtsa-2025-jun-dec.csv, deduplicates, filters to Driver/Operator Type = "None", writes incidents.json
- `index.html` — the interactive tool
- `incidents.json` — generated data file

### The two sections of the tool

**1. Incident browser.** A table of all incidents, filterable by company. Columns: company, date, city/state, crash-with, speed, severity, narrative (truncated, expandable). This addresses "having a nice way to browse the data."

**2. Rate estimator with sliders.** For each company, estimate miles per incident with a credible interval. The uncertain parameters controlled by sliders:

For Tesla:
- Total robotaxi miles in the period (default ~456k from robotaxitracker, slider range maybe 300k–600k)
- Fraction of post-Sep-1 miles with empty driver's seat (unknown; slider 0%–100%, the key uncertainty)

For Waymo:
- Total driverless miles in the period (default ~50M, slider to adjust)

For Zoox:
- Total driverless miles in the period (default ~500k, slider to adjust)

No fault weighting for now — all incidents count equally.

### Statistical model

Given k incidents in m miles, model the incident rate λ as Poisson. With a Jeffreys prior (Gamma(0.5, 0)), the posterior is Gamma(k + 0.5, m). From this we get:
- Point estimate of miles per incident: m / (k + 0.5)
- Credible interval for λ, inverted to get a credible interval for miles-per-incident

This is straightforward to compute in JS using the gamma distribution quantile function (inverse CDF). We can use jstat or implement the gamma quantile directly (it's a few dozen lines).

The output for each company: a visual display (inspired by aifuturesmodel.com) showing the estimated miles-per-incident with a 90% credible interval, updating live as sliders move.

